# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YDWBsjmMGeSKqCy9WUet08SUpdSBsFR1
"""

!git clone https://github.com/TonyBY/RAMQA.git

from google.colab import drive
drive.mount('/content/drive')

!pip install -r /content/RAMQA/src/requirements.txt

!pip install --no-deps -r /content/RAMQA/src/requirements.txt

!pip install jsonlines==4.0.0 lightning==2.1.0 pandas==2.1.2 peft==0.6.0 pickle5==0.0.11 pyserini==0.22.1 scipy==1.11.3 sentencepiece==0.1.99 spacy==3.7.2 tensorboard==2.15.0 torch==2.1.0 torchvision==0.16.0 tqdm==4.66.1 transformers==4.34.1 trl==0.7.4 unsloth==2024.4 wordcloud==1.9.2 word2number==1.1

!pip install unsloth==2024.4

!pip install wordcloud==1.9.2

!pip install word2number==1.1

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/RAMQA/src/utils/data_loader.py
# 
# import pandas as pd
# import os
# 
# def load_data(file_path):
#     """
#     Utility function to load data from a CSV or JSON file.
#     """
#     if not os.path.exists(file_path):
#         raise FileNotFoundError(f"Data file not found: {file_path}")
# 
#     file_extension = file_path.split('.')[-1]
# 
#     if file_extension == 'csv':
#         return pd.read_csv(file_path)
#     elif file_extension == 'json':
#         return pd.read_json(file_path)
#     else:
#         raise ValueError(f"Unsupported file type: {file_extension}")

import sys
sys.path.append('/content/RAMQA/src')

from utils.data_loader import load_data

!pip install transformers

from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments

# Since the zip file is already extracted, let's inspect the contents of the directory structure again to ensure we are working with the right files.

# Listing files in the 'RAMQA-main' directory to verify
ramqa_folder_path = '/content/RAMQA'

ramqa_files = os.listdir(ramqa_folder_path)
ramqa_files

# Let's correct the file path and proceed with listing the files in the 'src' directory.
ramqa_main_folder_path = '/content/RAMQA'

# Now let's list files in the 'src' directory
src_folder_path = os.path.join(ramqa_main_folder_path, 'src')
src_files = os.listdir(src_folder_path)
src_files

# Let's now open the 'RAMLLaMA' folder and list the files inside to locate 'train_RAMLLaMA.py'.
ramllama_folder_path = os.path.join(src_folder_path, 'RAMLLaMA')

# List files in the 'RAMLLaMA' directory
ramllama_files = os.listdir(ramllama_folder_path)
ramllama_files

"""## **Replacing unsloth with Hugging Face Transformers**"""

# Ensure the directory exists before saving the file
os.makedirs('/mnt/data', exist_ok=True)

# Define the correct path to save the updated file
updated_ramllama_path = '/mnt/data/updated_train_RAMLLaMA.py'

# Save the updated code to the correct file path
with open(updated_ramllama_path, 'w') as file:
    file.write(updated_ramllama_code)

updated_ramllama_path  # Return the path to the updated file

# Let's read and display the contents of the updated 'train_RAMLLaMA.py' file to check the modifications.
with open(train_ramllama_path, 'r') as file:
    updated_ramllama_code = file.read()

updated_ramllama_code[:2000]  # Display the first 2000 characters of the file content for inspection

# Display the next portion of the file content after the first 2000 characters.
updated_ramllama_code[2000:4000]  # Display characters from position 2000 to 4000 for inspection

# Define the folder path in Google Colab environment where you want to store the file
folder_path = '/content/RAMQA/src/RAMLLaVA'

# Ensure the folder exists
os.makedirs(folder_path, exist_ok=True)

# Define the full path for the file
file_path = os.path.join(folder_path, 'train_RAMLLaMA_updated.py')

# Write the code to the file
with open(file_path, 'w') as file:
    file.write(code)

file_path  # Return the path to the file in the specified folder in Colab

# Define the path where the file should be saved
file_path = '/mnt/data/train_RAMLLaMA_updated.py'

# Define the code as a multi-line string (you can paste the entire code here)
code = """
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals

import os
import sys

pwd = os.getcwd()
sys.path.append('/'.join(pwd.split('/')[:-6]))
sys.path.append('/'.join(pwd.split('/')[:-5]))
sys.path.append('/'.join(pwd.split('/')[:-4]))
sys.path.append('/'.join(pwd.split('/')[:-3]))
sys.path.append('/'.join(pwd.split('/')[:-2]))
sys.path.append('/'.join(pwd.split('/')[:-1]))

os.environ[
        "TORCH_DISTRIBUTED_DEBUG"
    ] = "DETAIL"  # set to DETAIL for runtime logging.
os.environ['FLAGS_eager_delete_tensor_gb'] = '0'  # enable gc
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

from datasets import Dataset, DatasetDict
import numpy as np
import logging
from PIL import ImageFile
import pathlib
import random

import torch
import transformers
from transformers import TrainingArguments, EarlyStoppingCallback

from trl import SFTTrainer

from transformers import AutoModelForSequenceClassification, AutoTokenizer

from RAMQA.src.utils.config import parser
from RAMQA.src.utils.args import prepare_logger
from RAMQA.src.utils.data_utils import read_jsonl, make_directory

np.set_printoptions(precision=4)
ImageFile.LOAD_TRUNCATED_IMAGES = True

transformers.logging.set_verbosity_error()
os.environ['FLAGS_eager_delete_tensor_gb'] = '0'  # enable gc

args = parser.parse_args()
logger = logging.getLogger()

torch.cuda.empty_cache()
torch.set_float32_matmul_precision('medium')


def train(args):
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)

    # 1. Load Dataset
    training_data = read_jsonl(args.train_file)
    val_data = read_jsonl(args.development_file)

    if args.debug:
        training_data = training_data[:(int(args.train_batch_size) * int(args.accumulate_gradients))]
        val_data = val_data[:int(args.train_batch_size)]

    traing_dataset = Dataset.from_list(training_data)
    val_dataset = Dataset.from_list(val_data)

    dataset = DatasetDict({
                        'train': traing_dataset,
                        'validation': val_dataset,
                    })

    logger.info(f"dataset: {dataset}")

    total_steps = len(training_data) // (int(args.train_batch_size) * int(args.accumulate_gradients))
    eval_steps = total_steps // args.patience

    logger.info(f"len(train_dataset): {len(training_data)}")
    logger.info(f"int(args.train_batch_size): {int(args.train_batch_size)}")
    logger.info(f"int(args.accumulate_gradients): {int(args.accumulate_gradients)}")
    logger.info(f"total_steps / Epoch: {total_steps}")
    logger.info(f"args.patience: {args.patience}")
    logger.info(f"eval_steps: {eval_steps}")


    # 2. Load Llama3 model
    model, tokenizer = AutoModelForSequenceClassification.from_pretrained(
        model_name = args.llama_model_name,
        max_seq_length = args.max_seq_len,
        dtype = None,
        load_in_4bit = args.bits == 4,
    )

    # 3 Before training
    def generate_text(text):
        inputs = tokenizer(text, return_tensors="pt").to("cuda:0")
        outputs = model.generate(**inputs, max_new_tokens=100)
        logger.info(tokenizer.decode(outputs[0], skip_special_tokens=True))

    logger.info("Before training\n")
    generate_text(val_dataset[0]['text'].split('### Response:')[0] + '### Response:\n        ')

    # 4. Do model patching and add fast LoRA weights and training
    model = AutoModelForSequenceClassification.get_peft_model(
                                                model,
                                                r = args.lora_r,
                                                target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                                                                "gate_proj", "up_proj", "down_proj",],
                                                lora_alpha = args.lora_alpha,
                                                lora_dropout = args.lora_dropout, # Supports any, but = 0 is optimized
                                                bias = "none",    # Supports any, but = "none" is optimized
                                                use_gradient_checkpointing = args.gradient_checkpointing,
                                                random_state = args.seed,
                                                max_seq_length = args.max_seq_len,
                                                use_rslora = False,  # Rank stabilized LoRA
                                                loftq_config = None, # LoftQ
                                            )

    # Set training parameters
    training_arguments = TrainingArguments(
        output_dir=args.output_dir,
        evaluation_strategy="steps",
        num_train_epochs=1 if args.debug else args.num_train_epochs,
        per_device_train_batch_size=args.train_batch_size,
        per_device_eval_batch_size=int(1 * args.train_batch_size),
        gradient_accumulation_steps=args.accumulate_gradients,
        # optim='paged_adamw_32bit',
        optim="adamw_8bit",
        eval_steps=1 if args.debug else eval_steps,
        save_steps=1 if args.debug else eval_steps, # Save checkpoint every X updates steps
        save_total_limit=1,
        eval_delay=0,
        # logging_steps=1 if args.debug else 100,
        logging_steps=1,
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay,
        bf16=True if torch.cuda.is_bf16_supported() else False,
        fp16=False if torch.cuda.is_bf16_supported() else True,
        max_grad_norm=0.3,                          # Maximum gradient normal (gradient clipping)
        max_steps=-1,                               # Number of training steps (overrides num_train_epochs)
        # warmup_ratio=0.03,                          # Ratio of steps for a linear warmup (from 0 to learning rate)
        warmup_steps = args.warmup_steps,
        group_by_length=True,                       # Group sequences into batches with same length; Saves memory and speeds up training considerably
        # lr_scheduler_type='cosine',                 # Learning rate schedule
        lr_scheduler_type='linear',                 # Learning rate schedule
        report_to="wandb",
        # report_to="tensorboard",
        load_best_model_at_end=True,
        gradient_checkpointing=args.gradient_checkpointing,
        gradient_checkpointing_kwargs={'use_reentrant': args.use_reentrant},
        disable_tqdm=False,
        seed=args.seed,
        # hub_token=args.huggingface_token,
    )

    # Set supervised fine-tuning parameters
    trainer = SFTTrainer(
        model=model,
        train_dataset=dataset["train"],
        eval_dataset=dataset["validation"],
        dataset_text_field="text",
        max_seq_length=args.max_seq_len,
        tokenizer=tokenizer,
        args=training_arguments,
    )

    trainer.add_callback(EarlyStoppingCallback(args.patience, 0.0))

    logger.info(f"trainer.args: {trainer.args}")

    # Train model
    logger.info("Start training...")
    if list(pathlib.Path(args.output_dir).glob("checkpoint-*")) and not args.debug:
        trainer.train(resume_from_checkpoint=True)
    else:
        trainer.train()

    # 5. After training
    logger.info("\n ######## \nAfter training\n")
    generate_text(val_dataset[0]['text'].split('### Response:')[0] + '### Response:\n        ')

    # Save trained model
    trainer.save_state()
    trainer.model.save_pretrained(os.path.join(args.output_dir, str(args.llama_model_name).split('/')[-1] + '_ramqa'))

if __name__=='__main__':
    make_directory(args.output_dir)
    prepare_logger(logger, debug=args.debug, save_to_file=os.path.join(args.output_dir, "llama3_ramqa_training.log"))
    logger.info(args)
    train(args)
    logger.info("ALL DONE!")
"""

# Write the code to the file
with open(file_path, 'w') as file:
    file.write(code)

# Return the file path for user download
file_path

"""## **Mixed Precision Training**"""

from torch.cuda.amp import autocast, GradScaler

import os

# Define the folder path where the file will be stored in Colab
folder_path = '/content/RAMQA/src/RAMLLaVA'

# Ensure the folder exists
os.makedirs(folder_path, exist_ok=True)

# Define the full path for the new file
file_path = os.path.join(folder_path, 'updated_train_RAMLLaMA.py')

# Write the code into the new Python file
with open(file_path, 'w') as file:
    file.write("""
import torch
from torch.cuda.amp import autocast, GradScaler
from transformers import AdamW, AutoModelForSequenceClassification, AutoTokenizer
from datasets import Dataset

# Initialize GradScaler for mixed precision
scaler = GradScaler()

# Initialize your model and optimizer
model = AutoModelForSequenceClassification.from_pretrained(args.llama_model_name)
model.to(device)  # Ensure the model is moved to the GPU
optimizer = AdamW(model.parameters(), lr=args.learning_rate)

# Training loop with mixed precision
for epoch in range(args.num_train_epochs):
    model.train()

    for batch in train_dataloader:
        # Move data to GPU
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        # Forward pass with mixed precision (autocast ensures FP16 where possible)
        with autocast():  # Use mixed precision in forward pass
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss

        # Backward pass with scaled loss (GradScaler scales gradients for FP16)
        scaler.scale(loss).backward()  # Scales the loss to prevent underflow

        # Optimizer step
        scaler.step(optimizer)  # Step the optimizer with scaled gradients
        scaler.update()  # Update the scaler for next iteration

        # Zero gradients after optimizer step
        optimizer.zero_grad()

    # Log training loss for the current epoch
    logger.info(f"Epoch {epoch} - Loss: {loss.item()}")

# After training, save the model
model.save_pretrained(args.output_dir)
""")

file_path  # Return the path to the new file for access in Colab

"""## **Modular Code**"""

import os

# Define the base directory and folder paths
base_dir = '/content/RAMQA/src/RAMLLaVA'

# Ensure the directory exists
os.makedirs(base_dir, exist_ok=True)

# model.py: Model architecture definition
model_code = """
from transformers import AutoModelForSequenceClassification, AutoTokenizer

def build_model(model_name, max_seq_len, load_in_4bit=False):
    \"\"\"
    Builds and returns the model and tokenizer.
    \"\"\"
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        max_seq_length=max_seq_len,
        load_in_4bit=load_in_4bit
    )
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    return model, tokenizer
"""

# data_processing.py: Data loading and preprocessing
data_processing_code = """
from datasets import Dataset

def load_data(train_file, val_file, batch_size, debug=False):
    \"\"\"
    Loads training and validation datasets, and prepares them for training.
    \"\"\"
    training_data = read_jsonl(train_file)
    val_data = read_jsonl(val_file)

    if debug:
        training_data = training_data[:(batch_size * 2)]  # Reduce data for debugging
        val_data = val_data[:batch_size]

    train_dataset = Dataset.from_list(training_data)
    val_dataset = Dataset.from_list(val_data)

    return train_dataset, val_dataset
"""

# train.py: Training loop with mixed precision
train_code = """
import torch
from torch.cuda.amp import autocast, GradScaler
from transformers import AdamW
from model import build_model
from data_processing import load_data

def train_model(args):
    \"\"\"
    Handles model training with mixed precision.
    \"\"\"
    scaler = GradScaler()

    # Load data and model
    train_dataset, val_dataset = load_data(args.train_file, args.development_file, args.train_batch_size, args.debug)
    model, tokenizer = build_model(args.llama_model_name, args.max_seq_len, args.bits == 4)

    model.to(args.device)
    optimizer = AdamW(model.parameters(), lr=args.learning_rate)

    for epoch in range(args.num_train_epochs):
        model.train()

        for batch in train_dataset:
            input_ids = batch['input_ids'].to(args.device)
            attention_mask = batch['attention_mask'].to(args.device)
            labels = batch['labels'].to(args.device)

            with autocast():
                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                loss = outputs.loss

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            optimizer.zero_grad()

        print(f"Epoch {epoch+1} completed")

    model.save_pretrained(args.output_dir)
"""

# evaluate.py: Evaluation logic
evaluate_code = """
from sklearn.metrics import accuracy_score

def evaluate_model(model, val_dataset, tokenizer, device):
    \"\"\"
    Evaluate the model on the validation set.
    \"\"\"
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for batch in val_dataset:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits

            preds = torch.argmax(logits, dim=-1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(all_labels, all_preds)
    print(f"Validation Accuracy: {accuracy * 100:.2f}%")
"""

# utils.py: Logging and config setup
utils_code = """
import logging

def setup_logging():
    \"\"\"
    Sets up logging for training and evaluation.
    \"\"\"
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger()
    return logger

def setup_config():
    \"\"\"
    Sets up the configuration dictionary with default values for training.
    \"\"\"
    config = {
        'train_batch_size': 16,
        'num_train_epochs': 3,
        'learning_rate': 5e-5,
        'output_dir': './output',
        'llama_model_name': 'bert-base-uncased',
        'max_seq_len': 128,
        'bits': 16,
        'device': 'cuda' if torch.cuda.is_available() else 'cpu',
        'train_file': './data/train.json',
        'development_file': './data/val.json'
    }
    return config
"""

# main.py: Main entry point to tie everything together
main_code = """
from train import train_model
from utils import setup_logging, setup_config

logger = setup_logging()

# Setup config
config = setup_config()

logger.info("Starting model training...")
train_model(config)
logger.info("Training completed.")
"""

# model.py
with open(os.path.join(base_dir, 'model.py'), 'w') as file:
    file.write(model_code)

# data_processing.py
with open(os.path.join(base_dir, 'data_processing.py'), 'w') as file:
    file.write(data_processing_code)

# train.py
with open(os.path.join(base_dir, 'train.py'), 'w') as file:
    file.write(train_code)

# evaluate.py
with open(os.path.join(base_dir, 'evaluate.py'), 'w') as file:
    file.write(evaluate_code)

# utils.py
with open(os.path.join(base_dir, 'utils.py'), 'w') as file:
    file.write(utils_code)

# main.py
with open(os.path.join(base_dir, 'main.py'), 'w') as file:
    file.write(main_code)

folder_path

"""## **Hugging Face Transformers**"""

# Replace this:
# from unsloth import FastLanguageModel

# With Hugging Face:
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# Old code using `unsloth`:
# model = FastLanguageModel.from_pretrained('model_name')

# New code using Hugging Face:
model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')

# Old code using `unsloth`:
# tokenizer = FastLanguageModelTokenizer.from_pretrained('model_name')

# New code using Hugging Face:
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

from transformers import AutoModelForSequenceClassification, AutoTokenizer

# Example: Loading a pre-trained model and tokenizer
model_name = 'bert-base-uncased'
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# For example, let's tokenize some input text
text = "Hello, how are you?"
inputs = tokenizer(text, return_tensors="pt")

# Forward pass with the model
outputs = model(**inputs)
print(outputs)

# Clone the GitHub repository to your Colab environment
!git clone https://github.com/aaishajalal/RAMQA_updated  # Replace with your repository URL

import shutil
import os

# Define the path to your repository and the folder you want to upload
repo_folder = '/content/RAMQA'  # This is where the repository is cloned
source_folder = '/content/RAMQA_updated'  # This is the folder with the files you want to upload

# Copy everything from your source folder into the cloned repo folder
shutil.copytree(source_folder, os.path.join(repo_folder, 'RAMQA_updated'))

# Verify if the files are copied
os.listdir(os.path.join(repo_folder, 'RAMQA_updated'))

# Commented out IPython magic to ensure Python compatibility.
# Navigate to your cloned repository
# %cd /content/RAMQA

# Add all the changes (new files, folders, etc.)
!git add .

# Commit the changes
!git commit -m "Added all project files to GitHub repository"

# Push the changes to GitHub
!git push origin main  # or master if that's your default branch

# Create a ZIP file of the entire repository
!zip -r /content/RAMQA_updated.zip /content/RAMQA

# Download the ZIP file from Colab
from google.colab import files
files.download('/content/RAMQA_updated.zip')

